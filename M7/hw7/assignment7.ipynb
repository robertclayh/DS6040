{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540a0931-2d8e-4e91-9f6b-1687fe31d514",
   "metadata": {},
   "source": [
    "# Assignment 7\n",
    "\n",
    "In this assignment, we'll use our two variational inference algorithms, and compare their speed and output to MCMC.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Please complete this Jupyter notebook and **don't** convert it to a `.py` file. Upload this notebook, along with any `.stan` files and any data sets as a `zip` file to Gradescope. Your work will be manually graded by our TA. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee6059c-b60a-4508-9e84-d947cb9596f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from cmdstanpy import CmdStanModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e16f3c-eda3-497a-b720-9f82c136d046",
   "metadata": {},
   "source": [
    "### Problem 1: A Derivation\n",
    "\n",
    "In lecture it was mentioned that the KL-divergence is an intractable quantity. However, the **Evidence Lower Bound (ELBO)**  is usually not. Your goal is to show that this.\n",
    "\n",
    "Show\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{KL}(g || p) \n",
    "&= - \\mathbb{E}_{g}\\left[ \\log\\frac{  L(y  \\mid \\theta) \\pi(\\theta \\mid y)}{g(\\theta \\mid \\phi)} \\right] +  \\text{constant}  \n",
    "\\end{align*}\n",
    "\n",
    "Or in other words, show that  the ELBO--the expectation without the negative sign on the right hand side--doesn't involve the unknown normalizing constant $p(y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ef08e-8b6f-4940-9eaa-ab2efa7f89e3",
   "metadata": {},
   "source": [
    "### Problem 2: Multinomial Regression\n",
    "\n",
    "Bayes' theorem looks a little different with supervised learning. First, data is split up into predictors (i.e. $x$) and dependent variables (call them $y$). In this example, we will also call our parameters $\\beta$, instead of $\\theta$. \n",
    "\n",
    "So, Bayes' rule will look like this\n",
    "\n",
    "$$\n",
    "\\pi(\\beta \\mid y, x) \\propto L(y \\mid \\beta, x) \\pi(\\beta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76faca1a-c59b-49a2-9b7b-4845e9a10009",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "Prove the above using rules of conditioning and any other required assumptions. Attach a screenshot of your deriation to this notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c9193-0c3b-49eb-978d-f3894b24fb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cf40e40-c4a2-4228-86ee-3407ce2190ce",
   "metadata": {},
   "source": [
    "Previously, we modeled categorical output with a multinomial distribution. Our previous notation for one row of data was as follows:\n",
    "\n",
    "Let $y = (y_1, y_2, \\ldots, y_k)$ be a vector of counts.\n",
    "\n",
    "We assume that there is a known total count (which means $\\sum_i y_i = n$). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788265e1-9e7b-45bf-ad26-a4d55ec19fce",
   "metadata": {},
   "source": [
    "A special case of the multinomial distribution--when $\\sum_i y_i$ is $1$--is the **Categorical Distribution**. \n",
    "\n",
    "This was just like the distinction between Bernoulli and Binomial random variables. A Bernoulli is when there is one trial, and a Binomial is when there are many. Here it is the same. A Categorical random variable is when there is only one trial, and a multinomial is more general and allows for multiple trails. Each trial here, though, each categorical realization, can have multiple outcomes!\n",
    "\n",
    "Because there is only one trial for each categorical realization, there are multiple ways to represent it ina program. \n",
    "\n",
    "One way is the **one-hot-encoding**. It matches the notation that we were using above. We could represent each realization as a length $k$ vector, and only allow one of the elements to be $1$ (or \"hot\").\n",
    "\n",
    "A more memory-efficient way is to just store the index/number of the category that took place (e.g. $3$). That's what we are doing with our data set. This way, we only need one column to store the dependent data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256c41f-4fa4-44eb-bc07-987087b15bd4",
   "metadata": {},
   "source": [
    "2.\n",
    "\n",
    "Suppose each message on a social media platform can have four possible sentiments: \"outrage\", \"joking\", \"intrigued\" and \"bored\". Suppose you are looking at three tweets. The first is outrage, the second is bored, and the third is intrigued. \n",
    "\n",
    "Write out two ways that you could represent this data. Describe your category ordering and index labeling. Does your counting start from $1$ or $0$. Are your categories ordered alphabetically? Use the one-hot encoding as well as the simple category labels.  Attach a screenshot of your deriation to this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b25d00-4975-4d50-93bf-1dc78ef206f8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8f74efe-ec01-4f2e-9a35-b4435485d939",
   "metadata": {},
   "source": [
    "Previously, our data were so simplistic that we assumed each observation--each row of data (usually)--was from the same distribution. In other words, the probabilities of each \"bucket\" stay the same. We wrote it like this\n",
    "\n",
    "Let $\\theta = (\\theta_1, \\theta_2, \\ldots, \\theta_k)$ be the probabilities of any trial resulting in each of the $k$ outcomes. \n",
    "\n",
    "We also assume the only possible outcomes are these $k$ outcomes so $\\sum_i \\theta_i = 1$.\n",
    "\n",
    "One way we can generalize this model is to allow the probabilities to vary. We could let them be affected by predictors. That's what we'll do in this homework. We will take our predictors, multiple them with some parameters, then use a nonlinear function to get the probabilities for each bucket. \n",
    "\n",
    "**Please note that we will describe the details of this model in more detail in a future module!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f272fada-3631-46f9-babe-f63538435c7d",
   "metadata": {},
   "source": [
    "Here is our high-level notation:\n",
    "\n",
    " - $N$: the number of observations/rows we have\n",
    " - $i$: the row number...goes from $1$ to $N$\n",
    " - $K$: number of categories/buckets we try to predict\n",
    " - $y_i$: the categorical dependent variable in row $i$ (it can be $1, \\ldots, K$)\n",
    " - $y$ is the set of all $y_1, \\ldots, y_N$\n",
    " - $D$: the number of predictors we can use to inform bucket probabilities\n",
    " - $x_i$: the row of predictor information. It has length $D$\n",
    " - $x$ is the matrix of all row predictors. It has shape $N \\times D$\n",
    "\n",
    "To drive this home, let's look at our specific data set, and try to make it look like this notation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0649af30-6032-4f14-baa3-862625872ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2\n",
      "1    1\n",
      "2    3\n",
      "3    3\n",
      "4    3\n",
      "Name: Y, dtype: int64\n",
      "\n",
      "\n",
      "unique y values:  [1 2 3 4] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.087147</td>\n",
       "      <td>-1.081342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.722566</td>\n",
       "      <td>-1.583863</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.179190</td>\n",
       "      <td>0.971790</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.159752</td>\n",
       "      <td>0.502624</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.727118</td>\n",
       "      <td>1.375704</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2  intercept\n",
       "0 -0.087147 -1.081342          1\n",
       "1 -0.722566 -1.583863          1\n",
       "2  0.179190  0.971790          1\n",
       "3 -1.159752  0.502624          1\n",
       "4 -0.727118  1.375704          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv(\"SoftmaxRegData1.csv\")\n",
    "y = d['Y']\n",
    "x = d[['X1','X2']].assign(intercept=1)\n",
    "print(y.head())\n",
    "print(\"\\n\\nunique y values: \", np.unique(y), \"\\n\\n\")\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651be72c-97b8-425d-b3a9-3c63e114a652",
   "metadata": {},
   "source": [
    "Our parameters of the model are \"weights\" that we weight predictors by. \n",
    "\n",
    "For parsimony, we use the same weights for each row of data. \n",
    "\n",
    "Also, each column of $\\beta$ will take data an give a probability output *for each possible outcome.* \n",
    "\n",
    "This makes the weight matrix $\\beta$ a $D \\times K$. For us, that's $3 \\times 4$.\n",
    "\n",
    "We can write our likelihood in a few different ways.\n",
    "\n",
    "$$\n",
    "L(y \\mid x, \\beta) = \\prod_{i=1}^N L(y_i \\mid x_i, \\beta)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "L(y_i \\mid x_i, \\beta) = \\text{Categorical}(\\text{softmax}(x_i ^\\intercal \\beta) )\n",
    "$$\n",
    "\n",
    "**We will discuss the softmax function more later.** Suffice it to say that $\\text{softmax}(x_i ^\\intercal \\beta)$ is a column vector of probabilities. Each probability describes the chances of each possible outome of the dependent variable $y_i$. Further, these probabilities depend on data for that particular row. They were kind of like our $\\theta$s from before, but they now depend on the data we have in a particular row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba2170a-a5d8-400c-87fa-311e238ada48",
   "metadata": {},
   "source": [
    "2.\n",
    "\n",
    "I have provided a `.stan` file called `multinomial_regression.stan` that describes the model above. Take a look at it, and observe how it maps to the above description of the model. After you are comfortable with it, build your model into an object called `model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8cf40c9e-0262-4168-9900-2f5d5612407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_code = ...\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29791f74-637c-473e-ae42-40550a0caef4",
   "metadata": {},
   "source": [
    "3. \n",
    "\n",
    "Use NUTS the MCMC algorithm to draw samples from this posterior. \n",
    "\n",
    "Do all the $\\hat{R}$s look good for each parameter? What are your parameter estimates? What are 90% credible intervals for those parameter estimates? Don't worry about writing too much...just call `.summary()` in a cell.\n",
    "\n",
    "Which parameters are very correlated (a posteriori) and how so? Why do you think is this the case? Justify your answer. Use `pd.plotting.scatter_matrix()` to visualize all the pairwise correlations of the posterior samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "021b29e6-ed68-456f-a8b6-b946d3c353f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_data = ...\n",
    "fit = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c044ec-fb89-4e31-a66b-088c6b2ebe0e",
   "metadata": {},
   "source": [
    "4. \n",
    "\n",
    "Use Pathfinder VI algorithm to obtain a posterior approximation. \n",
    "\n",
    "Are the parameter estimates close to the MCMC parameter estimates? \n",
    "\n",
    "Was it much faster than the MCMC sampler?\n",
    "\n",
    "Which parameters are very correlated (a posteriori) and how so? Why do you think is this the case? Use `pd.plotting.scatter_matrix()` to visualize all the pairwise correlations of the posterior samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d7898139-6eff-4d40-9a56-55af08609f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0744404-c48e-47c8-b557-44ad9c11a3bc",
   "metadata": {},
   "source": [
    "5. \n",
    "\n",
    "Use the ADVI algorithm to obtain a posterior approximation. \n",
    "\n",
    "Are the parameter estimates close to the pathfinder's estimates?\n",
    "\n",
    "Was it much faster than the MCMC sampler?\n",
    "\n",
    "Which parameters are very correlated (a posteriori) and how so? Why do you think is this the case? Use `pd.plotting.scatter_matrix()` to visualize all the pairwise correlations of the posterior samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "443f19a2-2eea-49cf-9f35-bcd5d2738da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit3 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c2fc9-8460-43e8-83c5-67d0922451fa",
   "metadata": {},
   "source": [
    "6.\n",
    "\n",
    "Describe two different approaches on how to simulate from the posterior predictive distribution. Why is this more interesting than simulating from the posterior preditive of our other models? \n",
    "\n",
    "Hint: did we make any modeling assumptions for the predictor data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2059f-ecb5-4184-bb71-c7ead19a7692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
